{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69fEbZJD0ewI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df0014fd-816c-46f3-c249-5550a6a45218"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deepcase\n",
            "  Downloading deepcase-1.0.1-py3-none-any.whl (37 kB)\n",
            "Installing collected packages: deepcase\n",
            "Successfully installed deepcase-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install deepcase\n",
        "from deepcase.preprocessing   import Preprocessor\n",
        "from deepcase.interpreter import Interpreter\n",
        "from deepcase.context_builder import ContextBuilder\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import traceback\n",
        "from sklearn.metrics import classification_report\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdG5yWvXpqtw",
        "outputId": "7da880cc-2385-4244-f9c6-7b8c4790d23a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ContextBuilder30ep Esistente\n"
          ]
        }
      ],
      "source": [
        "file_path = \"hdfs_test_normal.txt\"\n",
        "model_name = \"Interpeter\"\n",
        "context_builder = create_model_DEEPCASE(file_path, model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APy0iRRKpYJo"
      },
      "outputs": [],
      "source": [
        "def create_model_DEEPCASE(file_path, model_name):\n",
        "    try:\n",
        "        if os.path.exists(\"ContextBuilder30ep.save\"):\n",
        "            context_builder = ContextBuilder.load(\n",
        "                f\"ContextBuilder30ep.save\"\n",
        "            )\n",
        "\n",
        "            print(f\"ContextBuilder30ep Esistente\")\n",
        "            return context_builder\n",
        "        else:\n",
        "            print(f\"Il file {model_name} non esiste. Verra Generato il Modello!\")\n",
        "            preprocessor = Preprocessor(\n",
        "            length  = 10,    # 10 events in context\n",
        "            timeout = 86400, # Ignore events older than 1 day (60*60*24 = 86400 seconds)\n",
        "            )\n",
        "\n",
        "            context, events, labels, mapping = preprocessor.text(file_path, verbose=True)\n",
        "\n",
        "            if labels is None:\n",
        "                labels = np.full(events.shape[0], -1, dtype=int)\n",
        "\n",
        "            # Split into train and test sets (20:80) by time - assuming events are ordered chronologically\n",
        "            events_train_n  = events[:events.shape[0]//5 ]\n",
        "            events_test   = events[ events.shape[0]//5:]\n",
        "\n",
        "            context_train_n = context[:events.shape[0]//5 ]\n",
        "            context_test  = context[ events.shape[0]//5:]\n",
        "\n",
        "            label_train   = labels[:events.shape[0]//5 ]\n",
        "            label_test    = labels[ events.shape[0]//5:]\n",
        "\n",
        "            # context_train_n = normalize_tensor(context_train)\n",
        "            # events_train_n = normalize_tensor(events_train)\n",
        "            # context_test_n = normalize_tensor(context_test)\n",
        "            # events_test_n = normalize_tensor(events_test)\n",
        "            if os.path.exists(\"ContextBuilder.save\"):\n",
        "              context_builder = ContextBuilder.load(\n",
        "                  f\"ContextBuilder.save\"\n",
        "              )\n",
        "            else:\n",
        "\n",
        "              # Create ContextBuilder\n",
        "              context_builder = ContextBuilder(\n",
        "                  input_size    = 100,   # Number of input features to expect\n",
        "                  output_size   = 100,   # Same as input size\n",
        "                  hidden_size   = 128,   # Number of nodes in hidden layer, in paper we set this to 128\n",
        "                  max_length    = 10,    # Length of the context, should be same as context in Preprocessor\n",
        "              )\n",
        "\n",
        "              context_builder.fit(\n",
        "                  X             = context_train_n,               # Context to train with\n",
        "                  y             = events_train_n.reshape(-1, 1), # Events to train with, note that these should be of shape=(n_events, 1)\n",
        "                  epochs        = 30,                          # Number of epochs to train with\n",
        "                  batch_size    = 128,                         # Number of samples in each training batch, in paper this was 128\n",
        "                  learning_rate = 0.01,                        # Learning rate to train with, in paper this was 0.01\n",
        "                  verbose       = True,                        # If True, prints progress\n",
        "              )\n",
        "\n",
        "              context_builder.save('ContextBuilder.save')\n",
        "\n",
        "            ########################################################################\n",
        "            #                  Get prediction from ContextBuilder                  #\n",
        "            ########################################################################\n",
        "\n",
        "            # Use context builder to predict confidence\n",
        "            confidence, _ = context_builder.predict(\n",
        "                X = context_test\n",
        "            )\n",
        "\n",
        "            # Get confidence of the next step, seq_len 0 (n_samples, seq_len, output_size)\n",
        "            confidence = confidence[:, 0]\n",
        "            # Get confidence from log confidence\n",
        "            confidence = confidence.exp()\n",
        "            # Get prediction as maximum confidence\n",
        "            y_pred = confidence.argmax(dim=1)\n",
        "\n",
        "            ########################################################################\n",
        "            #                          Perform evaluation                          #\n",
        "            ########################################################################\n",
        "\n",
        "            # Get test and prediction as numpy array\n",
        "            y_test = events_test.cpu().numpy()\n",
        "            y_pred = y_pred     .cpu().numpy()\n",
        "\n",
        "            # Print classification report\n",
        "            print(classification_report(\n",
        "                y_true = y_test,\n",
        "                y_pred = y_pred,\n",
        "                digits = 4,\n",
        "            ))\n",
        "\n",
        "            # # Create Interpreter\n",
        "            # interpreter = Interpreter(\n",
        "            #     context_builder = context_builder, # ContextBuilder used to fit data\n",
        "            #     features        = 100,             # Number of input features to expect, should be same as ContextBuilder\n",
        "            #     eps             = 0.1,             # Epsilon value to use for DBSCAN clustering, in paper this was 0.1\n",
        "            #     min_samples     = 5,               # Minimum number of samples to use for DBSCAN clustering, in paper this was 5\n",
        "            #     threshold       = 0.2,             # Confidence threshold used for determining if attention from the ContextBuilder can be used, in paper this was 0.2\n",
        "            # )\n",
        "\n",
        "            # # Cluster samples with the interpreter\n",
        "            # clusters = interpreter.cluster(\n",
        "            #     X          = context_train_n,               # Context to train with\n",
        "            #     y          = events_train_n.reshape(-1, 1), # Events to train with, note that these should be of shape=(n_events, 1)\n",
        "            #     iterations = 100,                         # Number of iterations to use for attention query, in paper this was 100\n",
        "            #     batch_size = 1024,                        # Batch size to use for attention query, used to limit CUDA memory usage\n",
        "            #     verbose    = True,                        # If True, prints progress\n",
        "            # )\n",
        "\n",
        "\n",
        "            # # Compute scores for each cluster based on individual labels per sequence\n",
        "            # scores = interpreter.score_clusters(\n",
        "            #     scores   = label_train, # Labels used to compute score (either as loaded by Preprocessor, or put your own labels here)\n",
        "            #     strategy = \"max\",        # Strategy to use for scoring (one of \"max\", \"min\", \"avg\")\n",
        "            #     NO_SCORE = -1,           # Any sequence with this score will be ignored in the strategy.\n",
        "            #                             # If assigned a cluster, the sequence will inherit the cluster score.\n",
        "            #                             # If the sequence is not present in a cluster, it will receive a score of NO_SCORE.\n",
        "            # )\n",
        "\n",
        "            # # Assign scores to clusters in interpreter\n",
        "            # # Note that all sequences should be given a score and each sequence in the\n",
        "            # # same cluster should have the same score.\n",
        "            # interpreter.score(\n",
        "            #     scores  = scores, # Scores to assign to sequences\n",
        "            #     verbose = True,   # If True, prints progress\n",
        "            # )\n",
        "\n",
        "            # interpreter.save(f\"{model_name}\")\n",
        "            # # evalute_model_DEEPCASE(interpreter, file_path)\n",
        "            return True\n",
        "\n",
        "    except Exception as e:\n",
        "        # Gestisci altre eccezioni se si verificano\n",
        "        print(f\"Si è verificato un errore: {e}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = Preprocessor(length  = 10,timeout = 86400)\n",
        "context, events, labels, mapping = preprocessor.text(file_path, verbose=True)\n",
        "\n",
        "if labels is None:\n",
        "    labels = np.full(events.shape[0], -1, dtype=int)\n",
        "\n",
        "# Split into train and test sets (20:80) by time - assuming events are ordered chronologically\n",
        "events_train_n  = events[:events.shape[0]//5 ]\n",
        "events_test   = events[ events.shape[0]//5:]\n",
        "\n",
        "context_train_n = context[:events.shape[0]//5 ]\n",
        "context_test  = context[ events.shape[0]//5:]\n",
        "\n",
        "label_train   = labels[:events.shape[0]//5 ]\n",
        "label_test    = labels[ events.shape[0]//5:]\n",
        " # Use context builder to predict confidence\n",
        "confidence, _ = context_builder.predict(\n",
        "    X = context_test\n",
        ")\n",
        "\n",
        "# Get confidence of the next step, seq_len 0 (n_samples, seq_len, output_size)\n",
        "confidence = confidence[:, 0]\n",
        "# Get confidence from log confidence\n",
        "confidence = confidence.exp()\n",
        "# Get prediction as maximum confidence\n",
        "y_pred = confidence.argmax(dim=1)\n",
        "\n",
        "########################################################################\n",
        "#                          Perform evaluation                          #\n",
        "########################################################################\n",
        "\n",
        "# Get test and prediction as numpy array\n",
        "y_test = events_test.cpu().numpy()\n",
        "y_pred = y_pred     .cpu().numpy()\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(\n",
        "    y_true = y_test,\n",
        "    y_pred = y_pred,\n",
        "    digits = 4,\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSE585Paqog8",
        "outputId": "4e5d2594-8df3-476f-cb85-f0ca4e8ccd19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading: 100%|██████████| 302984/302984 [01:26<00:00, 3507.13it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6090    0.1832    0.2817     50178\n",
            "           1     0.8451    0.5529    0.6685    178358\n",
            "           2     0.5898    0.8079    0.6818    149575\n",
            "           3     0.8195    0.9997    0.9006    729051\n",
            "           4     0.6411    0.9057    0.7507      1124\n",
            "           5     0.9950    0.9873    0.9911    727926\n",
            "           6     0.9106    0.9366    0.9234    727926\n",
            "           7     0.8625    0.4791    0.6160      1100\n",
            "           8     0.9628    0.8474    0.9014      1101\n",
            "           9     0.0000    0.0000    0.0000        97\n",
            "          10     0.9990    0.9997    0.9993    593633\n",
            "          11     0.9994    0.3402    0.5077    242643\n",
            "          12     0.8905    0.9529    0.9207    592612\n",
            "          13     0.7429    0.3333    0.4602      1101\n",
            "          14     0.9382    0.9188    0.9284    730014\n",
            "          15     0.0000    0.0000    0.0000         2\n",
            "          16     0.0000    0.0000    0.0000         5\n",
            "\n",
            "    accuracy                         0.9039   4726446\n",
            "   macro avg     0.6944    0.6026    0.6195   4726446\n",
            "weighted avg     0.9110    0.9039    0.8946   4726446\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ohXRjN7Urr80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSSyn3gVqp9a"
      },
      "outputs": [],
      "source": [
        "def normalize_tensor(tensor_data):\n",
        "  tensor_min = torch.min(tensor_data)\n",
        "  tensor_max = torch.max(tensor_data)\n",
        "\n",
        "  # Normalizza il Tensor nell'intervallo [0, 1]\n",
        "  normalized_tensor = (tensor_data - tensor_min) / (tensor_max - tensor_min)\n",
        "\n",
        "  # Puoi anche normalizzare in un intervallo specifico, ad esempio [0, 99]\n",
        "  new_min = 0\n",
        "  new_max = 99\n",
        "  normalized_tensor_specific_range = \\\n",
        "   (tensor_data - tensor_min) / (tensor_max - tensor_min) \\\n",
        "        * (new_max - new_min) + new_min\n",
        "\n",
        "  return normalized_tensor_specific_range.to(torch.long)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}